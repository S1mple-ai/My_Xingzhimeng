"""
å¢å¼ºçš„æ—¥å¿—æŸ¥çœ‹ç•Œé¢
æä¾›ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†ã€åˆ†æå’Œç›‘æ§åŠŸèƒ½
"""

import streamlit as st
import os
import glob
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from collections import defaultdict, Counter

from logging_config import get_logger, get_log_statistics, set_log_level, LOG_LEVELS


class EnhancedLogViewer:
    """å¢å¼ºçš„æ—¥å¿—æŸ¥çœ‹å™¨"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = log_dir
        self.logger = get_logger()
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
    
    def render_log_management_panel(self):
        """æ¸²æŸ“æ—¥å¿—ç®¡ç†é¢æ¿"""
        st.header("ğŸ“‹ æ—¥å¿—ç®¡ç†ç³»ç»Ÿ")
        
        # åˆ›å»ºæ ‡ç­¾é¡µ
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š æ—¥å¿—æ¦‚è§ˆ", 
            "ğŸ” æ—¥å¿—æŸ¥çœ‹", 
            "ğŸ“ˆ æ—¥å¿—åˆ†æ", 
            "âš™ï¸ æ—¥å¿—é…ç½®", 
            "ğŸ§¹ æ—¥å¿—ç»´æŠ¤"
        ])
        
        with tab1:
            self._render_log_overview()
        
        with tab2:
            self._render_log_viewer()
        
        with tab3:
            self._render_log_analysis()
        
        with tab4:
            self._render_log_configuration()
        
        with tab5:
            self._render_log_maintenance()
    
    def _render_log_overview(self):
        """æ¸²æŸ“æ—¥å¿—æ¦‚è§ˆ"""
        st.subheader("ğŸ“Š æ—¥å¿—ç³»ç»Ÿæ¦‚è§ˆ")
        
        # è·å–æ—¥å¿—ç»Ÿè®¡
        stats = get_log_statistics()
        log_files = self._get_log_files()
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ“ æ—¥å¿—æ–‡ä»¶æ•°", stats['total_files'])
        
        with col2:
            st.metric("ğŸ’¾ æ€»å¤§å°", f"{stats['total_size_mb']:.2f} MB")
        
        with col3:
            # ä»Šæ—¥é”™è¯¯æ•°é‡
            today_errors = self._count_today_errors()
            st.metric("âŒ ä»Šæ—¥é”™è¯¯", today_errors)
        
        with col4:
            # æœ€æ–°æ—¥å¿—æ—¶é—´
            if stats['latest_modified']:
                latest_time = stats['latest_modified'].strftime("%H:%M:%S")
                st.metric("ğŸ• æœ€æ–°æ—¥å¿—", latest_time)
            else:
                st.metric("ğŸ• æœ€æ–°æ—¥å¿—", "æ— ")
        
        # æ—¥å¿—æ–‡ä»¶åˆ†å¸ƒå›¾è¡¨
        if stats['files_by_category']:
            st.subheader("ğŸ“Š æ—¥å¿—æ–‡ä»¶åˆ†å¸ƒ")
            
            categories = list(stats['files_by_category'].keys())
            counts = list(stats['files_by_category'].values())
            
            fig = px.pie(
                values=counts,
                names=categories,
                title="æ—¥å¿—æ–‡ä»¶ç±»å‹åˆ†å¸ƒ"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # æ—¥å¿—æ–‡ä»¶åˆ—è¡¨
        st.subheader("ğŸ“‹ æ—¥å¿—æ–‡ä»¶åˆ—è¡¨")
        if log_files:
            df = pd.DataFrame(log_files)
            df['å¤§å°(KB)'] = (df['size'] / 1024).round(2)
            df['ä¿®æ”¹æ—¶é—´'] = pd.to_datetime(df['modified']).dt.strftime('%Y-%m-%d %H:%M:%S')
            
            display_df = df[['name', 'å¤§å°(KB)', 'ä¿®æ”¹æ—¶é—´', 'category']].rename(columns={
                'name': 'æ–‡ä»¶å',
                'category': 'ç±»å‹'
            })
            
            st.dataframe(display_df, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— æ—¥å¿—æ–‡ä»¶")
    
    def _render_log_viewer(self):
        """æ¸²æŸ“æ—¥å¿—æŸ¥çœ‹å™¨"""
        st.subheader("ğŸ” æ—¥å¿—æŸ¥çœ‹å™¨")
        
        # æ—¥å¿—æ–‡ä»¶é€‰æ‹©
        log_files = self._get_log_files()
        if not log_files:
            st.info("ğŸ“ æš‚æ— æ—¥å¿—æ–‡ä»¶")
            return
        
        file_options = {f['name']: f for f in log_files}
        selected_file = st.selectbox(
            "é€‰æ‹©æ—¥å¿—æ–‡ä»¶",
            options=list(file_options.keys()),
            key="log_file_selector"
        )
        
        if not selected_file:
            return
        
        # è¿‡æ»¤é€‰é¡¹
        col1, col2, col3 = st.columns(3)
        
        with col1:
            log_levels = ["å…¨éƒ¨"] + list(LOG_LEVELS.keys())
            selected_level = st.selectbox("æ—¥å¿—çº§åˆ«", log_levels, key="log_level_filter")
        
        with col2:
            time_range = st.selectbox(
                "æ—¶é—´èŒƒå›´",
                ["å…¨éƒ¨", "æœ€è¿‘1å°æ—¶", "æœ€è¿‘6å°æ—¶", "æœ€è¿‘24å°æ—¶", "æœ€è¿‘7å¤©"],
                key="time_range_filter"
            )
        
        with col3:
            search_term = st.text_input("ğŸ” æœç´¢å…³é”®è¯", key="log_search_term")
        
        # æ˜¾ç¤ºé€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            max_lines = st.slider("æ˜¾ç¤ºè¡Œæ•°", 10, 1000, 100, key="log_max_lines")
        with col2:
            reverse_order = st.checkbox("å€’åºæ˜¾ç¤ºï¼ˆæœ€æ–°åœ¨å‰ï¼‰", value=True, key="log_reverse")
        
        # è¯»å–å’Œè¿‡æ»¤æ—¥å¿—
        log_content = self._read_and_filter_log(
            selected_file,
            selected_level,
            time_range,
            search_term,
            max_lines,
            reverse_order
        )
        
        if log_content:
            # ä¸‹è½½æŒ‰é’®
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ—¥å¿—",
                data=log_content,
                file_name=f"{selected_file}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
                mime="text/plain"
            )
            
            # æ˜¾ç¤ºæ—¥å¿—å†…å®¹
            st.code(log_content, language="text")
        else:
            st.info("ğŸ“ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—å†…å®¹")
    
    def _render_log_analysis(self):
        """æ¸²æŸ“æ—¥å¿—åˆ†æ"""
        st.subheader("ğŸ“ˆ æ—¥å¿—åˆ†æ")
        
        # åˆ†æé€‰é¡¹
        analysis_type = st.selectbox(
            "åˆ†æç±»å‹",
            ["é”™è¯¯ç»Ÿè®¡", "æ€§èƒ½åˆ†æ", "è®¿é—®æ¨¡å¼", "æ—¶é—´åˆ†å¸ƒ"],
            key="analysis_type"
        )
        
        if analysis_type == "é”™è¯¯ç»Ÿè®¡":
            self._render_error_analysis()
        elif analysis_type == "æ€§èƒ½åˆ†æ":
            self._render_performance_analysis()
        elif analysis_type == "è®¿é—®æ¨¡å¼":
            self._render_access_pattern_analysis()
        elif analysis_type == "æ—¶é—´åˆ†å¸ƒ":
            self._render_time_distribution_analysis()
    
    def _render_log_configuration(self):
        """æ¸²æŸ“æ—¥å¿—é…ç½®"""
        st.subheader("âš™ï¸ æ—¥å¿—é…ç½®")
        
        # å½“å‰é…ç½®æ˜¾ç¤º
        st.write("**å½“å‰æ—¥å¿—é…ç½®ï¼š**")
        current_level = st.session_state.get('current_log_level', 'INFO')
        st.info(f"æ—¥å¿—çº§åˆ«: {current_level}")
        
        # æ—¥å¿—çº§åˆ«è®¾ç½®
        st.write("**ä¿®æ”¹æ—¥å¿—çº§åˆ«ï¼š**")
        new_level = st.selectbox(
            "é€‰æ‹©æ–°çš„æ—¥å¿—çº§åˆ«",
            list(LOG_LEVELS.keys()),
            index=list(LOG_LEVELS.keys()).index(current_level),
            key="new_log_level"
        )
        
        if st.button("åº”ç”¨é…ç½®", key="apply_log_config"):
            set_log_level(new_level)
            st.session_state['current_log_level'] = new_level
            st.success(f"âœ… æ—¥å¿—çº§åˆ«å·²è®¾ç½®ä¸º: {new_level}")
            st.rerun()
        
        # æ—¥å¿—è½®è½¬é…ç½®
        st.write("**æ—¥å¿—è½®è½¬é…ç½®ï¼š**")
        col1, col2 = st.columns(2)
        
        with col1:
            max_file_size = st.number_input(
                "æœ€å¤§æ–‡ä»¶å¤§å° (MB)",
                min_value=1,
                max_value=100,
                value=10,
                key="max_file_size"
            )
        
        with col2:
            backup_count = st.number_input(
                "å¤‡ä»½æ–‡ä»¶æ•°é‡",
                min_value=1,
                max_value=10,
                value=5,
                key="backup_count"
            )
        
        if st.button("åº”ç”¨è½®è½¬é…ç½®", key="apply_rotation_config"):
            st.success("âœ… æ—¥å¿—è½®è½¬é…ç½®å·²æ›´æ–°")
    
    def _render_log_maintenance(self):
        """æ¸²æŸ“æ—¥å¿—ç»´æŠ¤"""
        st.subheader("ğŸ§¹ æ—¥å¿—ç»´æŠ¤")
        
        # æ¸…ç†é€‰é¡¹
        st.write("**æ—¥å¿—æ¸…ç†ï¼š**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            cleanup_days = st.number_input(
                "æ¸…ç†å¤šå°‘å¤©å‰çš„æ—¥å¿—",
                min_value=1,
                max_value=365,
                value=30,
                key="cleanup_days"
            )
        
        with col2:
            cleanup_size_mb = st.number_input(
                "æ¸…ç†è¶…è¿‡å¤šå°‘MBçš„æ—¥å¿—",
                min_value=1,
                max_value=1000,
                value=100,
                key="cleanup_size_mb"
            )
        
        # æ¸…ç†æŒ‰é’®
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç†æ—§æ—¥å¿—", key="cleanup_old_logs"):
                self._cleanup_old_logs(cleanup_days)
        
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç†å¤§æ–‡ä»¶", key="cleanup_large_files"):
                self._cleanup_large_files(cleanup_size_mb)
        
        with col3:
            if st.button("ğŸ—‘ï¸ æ¸…ç†æ‰€æœ‰æ—¥å¿—", key="cleanup_all_logs"):
                if st.session_state.get('confirm_cleanup_all', False):
                    self._cleanup_all_logs()
                    st.session_state['confirm_cleanup_all'] = False
                    st.success("âœ… æ‰€æœ‰æ—¥å¿—å·²æ¸…ç†")
                    st.rerun()
                else:
                    st.session_state['confirm_cleanup_all'] = True
                    st.warning("âš ï¸ å†æ¬¡ç‚¹å‡»ç¡®è®¤æ¸…ç†æ‰€æœ‰æ—¥å¿—")
        
        # æ—¥å¿—å‹ç¼©
        st.write("**æ—¥å¿—å‹ç¼©ï¼š**")
        if st.button("ğŸ“¦ å‹ç¼©æ—¥å¿—æ–‡ä»¶", key="compress_logs"):
            self._compress_logs()
    
    def _get_log_files(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—æ–‡ä»¶åˆ—è¡¨"""
        log_files = []
        
        if not os.path.exists(self.log_dir):
            return log_files
        
        for filename in os.listdir(self.log_dir):
            if filename.endswith('.log'):
                file_path = os.path.join(self.log_dir, filename)
                try:
                    stat = os.stat(file_path)
                    log_files.append({
                        'name': filename,
                        'path': file_path,
                        'size': stat.st_size,
                        'modified': datetime.fromtimestamp(stat.st_mtime),
                        'category': filename.replace('.log', '')
                    })
                except OSError:
                    continue
        
        return sorted(log_files, key=lambda x: x['modified'], reverse=True)
    
    def _count_today_errors(self) -> int:
        """ç»Ÿè®¡ä»Šæ—¥é”™è¯¯æ•°é‡"""
        error_log_path = os.path.join(self.log_dir, "error.log")
        if not os.path.exists(error_log_path):
            return 0
        
        today = datetime.now().date()
        error_count = 0
        
        try:
            with open(error_log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if 'ERROR' in line and today.strftime('%Y-%m-%d') in line:
                        error_count += 1
        except Exception:
            pass
        
        return error_count
    
    def _read_and_filter_log(self, filename: str, level: str, time_range: str, 
                           search_term: str, max_lines: int, reverse_order: bool) -> str:
        """è¯»å–å’Œè¿‡æ»¤æ—¥å¿—å†…å®¹"""
        file_path = os.path.join(self.log_dir, filename)
        
        if not os.path.exists(file_path):
            return ""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception:
            return ""
        
        # æ—¶é—´è¿‡æ»¤
        if time_range != "å…¨éƒ¨":
            lines = self._filter_by_time(lines, time_range)
        
        # çº§åˆ«è¿‡æ»¤
        if level != "å…¨éƒ¨":
            lines = [line for line in lines if level in line]
        
        # å…³é”®è¯æœç´¢
        if search_term:
            lines = [line for line in lines if search_term.lower() in line.lower()]
        
        # æ’åº
        if reverse_order:
            lines = lines[::-1]
        
        # é™åˆ¶è¡Œæ•°
        lines = lines[:max_lines]
        
        return ''.join(lines)
    
    def _filter_by_time(self, lines: List[str], time_range: str) -> List[str]:
        """æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤æ—¥å¿—è¡Œ"""
        now = datetime.now()
        
        if time_range == "æœ€è¿‘1å°æ—¶":
            cutoff = now - timedelta(hours=1)
        elif time_range == "æœ€è¿‘6å°æ—¶":
            cutoff = now - timedelta(hours=6)
        elif time_range == "æœ€è¿‘24å°æ—¶":
            cutoff = now - timedelta(days=1)
        elif time_range == "æœ€è¿‘7å¤©":
            cutoff = now - timedelta(days=7)
        else:
            return lines
        
        filtered_lines = []
        for line in lines:
            # å°è¯•ä»æ—¥å¿—è¡Œä¸­æå–æ—¶é—´æˆ³
            timestamp_match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line)
            if timestamp_match:
                try:
                    log_time = datetime.strptime(timestamp_match.group(), '%Y-%m-%d %H:%M:%S')
                    if log_time >= cutoff:
                        filtered_lines.append(line)
                except ValueError:
                    filtered_lines.append(line)  # å¦‚æœè§£æå¤±è´¥ï¼Œä¿ç•™è¯¥è¡Œ
            else:
                filtered_lines.append(line)  # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œä¿ç•™è¯¥è¡Œ
        
        return filtered_lines
    
    def _render_error_analysis(self):
        """æ¸²æŸ“é”™è¯¯åˆ†æ"""
        st.write("**é”™è¯¯ç»Ÿè®¡åˆ†æ**")
        
        error_log_path = os.path.join(self.log_dir, "error.log")
        if not os.path.exists(error_log_path):
            st.info("ğŸ“ æš‚æ— é”™è¯¯æ—¥å¿—")
            return
        
        # åˆ†æé”™è¯¯æ—¥å¿—
        error_stats = self._analyze_error_log(error_log_path)
        
        if error_stats:
            # é”™è¯¯ç±»å‹åˆ†å¸ƒ
            col1, col2 = st.columns(2)
            
            with col1:
                if error_stats['error_types']:
                    fig = px.bar(
                        x=list(error_stats['error_types'].keys()),
                        y=list(error_stats['error_types'].values()),
                        title="é”™è¯¯ç±»å‹åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                if error_stats['hourly_distribution']:
                    fig = px.line(
                        x=list(error_stats['hourly_distribution'].keys()),
                        y=list(error_stats['hourly_distribution'].values()),
                        title="é”™è¯¯æ—¶é—´åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            # æœ€è¿‘é”™è¯¯åˆ—è¡¨
            st.write("**æœ€è¿‘é”™è¯¯ï¼š**")
            if error_stats['recent_errors']:
                for error in error_stats['recent_errors'][:10]:
                    st.error(f"ğŸ• {error['time']} - {error['message']}")
        else:
            st.info("ğŸ“ æš‚æ— é”™è¯¯æ•°æ®")
    
    def _render_performance_analysis(self):
        """æ¸²æŸ“æ€§èƒ½åˆ†æ"""
        st.write("**æ€§èƒ½åˆ†æ**")
        
        perf_log_path = os.path.join(self.log_dir, "performance.log")
        if not os.path.exists(perf_log_path):
            st.info("ğŸ“ æš‚æ— æ€§èƒ½æ—¥å¿—")
            return
        
        # åˆ†ææ€§èƒ½æ—¥å¿—
        perf_stats = self._analyze_performance_log(perf_log_path)
        
        if perf_stats:
            # æ€§èƒ½æŒ‡æ ‡
            col1, col2, col3 = st.columns(3)
            
            with col1:
                avg_time = sum(perf_stats['execution_times']) / len(perf_stats['execution_times'])
                st.metric("å¹³å‡æ‰§è¡Œæ—¶é—´", f"{avg_time:.3f}s")
            
            with col2:
                max_time = max(perf_stats['execution_times'])
                st.metric("æœ€é•¿æ‰§è¡Œæ—¶é—´", f"{max_time:.3f}s")
            
            with col3:
                st.metric("æ€§èƒ½è®°å½•æ•°", len(perf_stats['execution_times']))
            
            # æ€§èƒ½è¶‹åŠ¿å›¾
            if perf_stats['time_series']:
                df = pd.DataFrame(perf_stats['time_series'])
                fig = px.line(df, x='time', y='execution_time', title="æ€§èƒ½è¶‹åŠ¿")
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— æ€§èƒ½æ•°æ®")
    
    def _render_access_pattern_analysis(self):
        """æ¸²æŸ“è®¿é—®æ¨¡å¼åˆ†æ"""
        st.write("**è®¿é—®æ¨¡å¼åˆ†æ**")
        st.info("ğŸš§ åŠŸèƒ½å¼€å‘ä¸­...")
    
    def _render_time_distribution_analysis(self):
        """æ¸²æŸ“æ—¶é—´åˆ†å¸ƒåˆ†æ"""
        st.write("**æ—¶é—´åˆ†å¸ƒåˆ†æ**")
        st.info("ğŸš§ åŠŸèƒ½å¼€å‘ä¸­...")
    
    def _analyze_error_log(self, file_path: str) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception:
            return {}
        
        error_types = Counter()
        hourly_distribution = defaultdict(int)
        recent_errors = []
        
        for line in lines:
            if 'ERROR' in line:
                # æå–é”™è¯¯ç±»å‹
                if 'Exception' in line:
                    error_type = re.search(r'(\w+Exception)', line)
                    if error_type:
                        error_types[error_type.group(1)] += 1
                
                # æå–æ—¶é—´
                timestamp_match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line)
                if timestamp_match:
                    try:
                        log_time = datetime.strptime(timestamp_match.group(), '%Y-%m-%d %H:%M:%S')
                        hourly_distribution[log_time.hour] += 1
                        
                        recent_errors.append({
                            'time': timestamp_match.group(),
                            'message': line.strip()
                        })
                    except ValueError:
                        pass
        
        # æŒ‰æ—¶é—´æ’åºæœ€è¿‘é”™è¯¯
        recent_errors.sort(key=lambda x: x['time'], reverse=True)
        
        return {
            'error_types': dict(error_types),
            'hourly_distribution': dict(hourly_distribution),
            'recent_errors': recent_errors
        }
    
    def _analyze_performance_log(self, file_path: str) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ—¥å¿—"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception:
            return {}
        
        execution_times = []
        time_series = []
        
        for line in lines:
            if 'PERFORMANCE' in line:
                # æå–æ‰§è¡Œæ—¶é—´
                time_match = re.search(r'(\d+\.\d+)s', line)
                if time_match:
                    exec_time = float(time_match.group(1))
                    execution_times.append(exec_time)
                    
                    # æå–æ—¶é—´æˆ³
                    timestamp_match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line)
                    if timestamp_match:
                        time_series.append({
                            'time': timestamp_match.group(),
                            'execution_time': exec_time
                        })
        
        return {
            'execution_times': execution_times,
            'time_series': time_series
        }
    
    def _cleanup_old_logs(self, days: int):
        """æ¸…ç†æ—§æ—¥å¿—"""
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_count = 0
        
        for log_file in self._get_log_files():
            if log_file['modified'] < cutoff_date:
                try:
                    os.remove(log_file['path'])
                    cleaned_count += 1
                except OSError:
                    pass
        
        st.success(f"âœ… å·²æ¸…ç† {cleaned_count} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")
    
    def _cleanup_large_files(self, size_mb: int):
        """æ¸…ç†å¤§æ–‡ä»¶"""
        size_bytes = size_mb * 1024 * 1024
        cleaned_count = 0
        
        for log_file in self._get_log_files():
            if log_file['size'] > size_bytes:
                try:
                    os.remove(log_file['path'])
                    cleaned_count += 1
                except OSError:
                    pass
        
        st.success(f"âœ… å·²æ¸…ç† {cleaned_count} ä¸ªå¤§æ—¥å¿—æ–‡ä»¶")
    
    def _cleanup_all_logs(self):
        """æ¸…ç†æ‰€æœ‰æ—¥å¿—"""
        log_files = glob.glob(os.path.join(self.log_dir, "*.log"))
        for log_file in log_files:
            try:
                os.remove(log_file)
            except OSError:
                pass
    
    def _compress_logs(self):
        """å‹ç¼©æ—¥å¿—æ–‡ä»¶"""
        st.info("ğŸš§ æ—¥å¿—å‹ç¼©åŠŸèƒ½å¼€å‘ä¸­...")


def render_enhanced_log_viewer():
    """æ¸²æŸ“å¢å¼ºçš„æ—¥å¿—æŸ¥çœ‹å™¨"""
    viewer = EnhancedLogViewer()
    viewer.render_log_management_panel()


# å¯¼å‡ºä¸»è¦æ¥å£
__all__ = ['EnhancedLogViewer', 'render_enhanced_log_viewer']